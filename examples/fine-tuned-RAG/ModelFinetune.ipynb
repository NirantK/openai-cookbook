{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning for RAG\n",
    "\n",
    "\n",
    "\n",
    "Here, we'll take you through the process, complete with code examples, to help you fine-tune your OpenAI model for usage with RAG like a pro.\n",
    "\n",
    "\n",
    "\n",
    "To begin, we've selected a dataset where we've a guarantee that the retrieval is perfect. We've selected a subset of the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset, which is a collection of questions and answers about Wikipedia articles. We've also included samples where the answer is not present in the context, to demonstrate how RAG handles this case.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setting up the Environment\n",
    "2. Data Preparation\n",
    "3. Running the Model\n",
    "4. Evaluation\n",
    "5. Fine-Tuning\n",
    "6. Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "### Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas openai tqdm tenacity pandarallel scikit-learn tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import openai\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "tqdm.pandas()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Load your data and take a quick look at the first few rows. Notice that we've included a few samples where the answer is not present in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"v2_1K_Seed=37_sample.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "\n",
    "# Initialize a local Qdrant Client\n",
    "qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "# Initialize a Qdrant Cloud Client\n",
    "# qdrant_client = QdrantClient(\n",
    "#     url=\"your_url\",\n",
    "#     api_key=\"your_api_key\",\n",
    "# )\n",
    "\n",
    "collection_name = \"cookbook\" # An arbitrary name for the collection\n",
    "\n",
    "# Create the collection\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.DOT),\n",
    ") # 384 is the size of the embedding. This is the default size for the embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed.embedding import DefaultEmbedding\n",
    "from typing import List\n",
    "\n",
    "# questions is a list of questions from the dataframe column \"question\"\n",
    "questions = df[\"question\"].tolist()\n",
    "\n",
    "# embeddings is a list of embeddings for each question\n",
    "embedding_model = DefaultEmbedding()\n",
    "embeddings: List[np.ndarray] = list(embedding_model.embed(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "\n",
    "# iterate through the dataframe and create a PointStruct object for each row, and append it to the points list\n",
    "for index, row in df.iterrows():\n",
    "    # convert numpy array to list\n",
    "    embedding = embeddings[index].tolist()\n",
    "\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=index,\n",
    "            vector=embedding,\n",
    "            payload={\n",
    "                \"question\": row[\"question\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"context\": row[\"context\"],\n",
    "                \"is_impossible\": row[\"is_impossible\"],\n",
    "                \"answers\": row[\"answers\"],\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert all the points to the collection\n",
    "operation_info = qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "print(operation_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the OpenAI Model for Question Answering\n",
    "\n",
    "### Prompt, API Call, and Answer\n",
    "Create functions to get prompt messages and make API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get prompt messages\n",
    "def get_prompt(row):\n",
    "    return [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n",
    "    Question: {row.question}\\n\\n\n",
    "    Context: {row.context}\\n\\n\n",
    "    Answer:\\n\"\"\"},\n",
    "    ]\n",
    "\n",
    "# Function with tenacity for retries\n",
    "@retry(wait=wait_exponential(multiplier=1, min=2, max=6))\n",
    "def api_call(messages, model):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        max_tokens=100,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "# Main function to answer question\n",
    "def answer_question(row, model=\"gpt-3.5-turbo-0613\"):\n",
    "    messages = get_prompt(row)\n",
    "    response = api_call(messages, model)\n",
    "    return response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use progress_apply with tqdm for progress bar\n",
    "df[\"generated_answer\"] = df.progress_apply(answer_question, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the results to a JSON file\n",
    "# df.to_json(\"1K_with_generated_answers.json\", lines=True, orient=\"records\") # Save to JSON"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To evaluate the model's performance, compare the predicted answer to the actual answers -- if any of the actual answers are present in the predicted answer, then it's a match. We've also created error categories to help you understand where the model is struggling.\n",
    "\n",
    "1. Expected and Right: The model responsded the correct answer. It may have also included other answers that were not in the context.\n",
    "2. Expected but \"IDK\": The model responded with \"I don't know\" (IDK) while the answer was present in the context. *This is a model error* and better than giving the wrong answer. We exclude this from the overall error rate.\n",
    "3. Expected but Wrong: The model responded with an incorrect answer. *This is a model ERROR.*\n",
    "4. Hallucination: The model responded with an answer, when \"I don't know\" was expected. **This is a model error.** \n",
    "5. Did not expect and IDK: The model responded with \"I don't know\" (IDK) and the answer was not present in the context. *This is a model WIN.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrixEvaluator:\n",
    "    def __init__(self, df, answers_column=\"generated_answer\"):\n",
    "        self.df = df\n",
    "        self.y_pred = []\n",
    "        self.labels = [\n",
    "            'Expected and Right', 'Expected but IDK', \n",
    "            'Expected but Wrong', 'Hallucination', \n",
    "            'Did not Expect and IDK'\n",
    "        ]\n",
    "        self.answers_column = answers_column\n",
    "    \n",
    "    def _evaluate_single_row(self, row):\n",
    "        is_impossible = row['is_impossible']\n",
    "        generated_answer = row[self.answers_column].lower()\n",
    "        actual_answers = [ans.lower() for ans in row['answers']]\n",
    "        \n",
    "        y_pred = (\n",
    "            'Expected and Right' if not is_impossible and any(ans in generated_answer for ans in actual_answers) else\n",
    "            'Expected but IDK' if not is_impossible and generated_answer == \"i don't know\" else\n",
    "            'Expected but Wrong' if not is_impossible and generated_answer not in actual_answers else\n",
    "            'Hallucination' if is_impossible and generated_answer != \"i don't know\" else\n",
    "            'Did not Expect and IDK'\n",
    "        )\n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate_answers(self):\n",
    "        self.y_pred = self.df.apply(self._evaluate_single_row, axis=1)\n",
    "    \n",
    "    def generate_matrices(self, use_percentages=False):\n",
    "        # Using value_counts to create a Series of frequencies, then reindexing to include missing labels with count 0\n",
    "        freq_series = self.y_pred.value_counts().reindex(self.labels, fill_value=0)\n",
    "        if use_percentages:\n",
    "            total = freq_series.sum()\n",
    "            freq_series = (freq_series / total * 100).apply(\"{0:.2f}%\".format)\n",
    "        return freq_series\n",
    "\n",
    "evaluator = ConfusionMatrixEvaluator(df, answers_column=\"generated_answer\")\n",
    "evaluator.evaluate_answers()\n",
    "error_categories = evaluator.generate_matrices(use_percentages=True)\n",
    "error_categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "### Prepare the Fine-Tuning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_jsonl(df):\n",
    "    def create_jsonl_entry(row):\n",
    "        answer = row['answers'][0] if row['answers'] else \"I don't know\"\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n",
    "            Question: {row.question}\\n\\n\n",
    "            Context: {row.context}\\n\\n\n",
    "            Answer:\\n\"\"\"},\n",
    "            {'role': 'assistant', 'content': answer}\n",
    "        ]\n",
    "        return json.dumps({'messages': messages})\n",
    "\n",
    "    jsonl_output = df.progress_apply(create_jsonl_entry, axis=1)\n",
    "    return \"\\n\".join(jsonl_output)\n",
    "\n",
    "with open(\"squad-stratified-1000-ft-v2.json\", \"w\") as f:\n",
    "    f.write(dataframe_to_jsonl(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Verify the Fine-Tuning Data\n",
    "\n",
    "The script below will verify that the data is in the format that OpenAI expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We start by importing the required packages\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# # Next, we specify the data path and open the JSONL file\n",
    "\n",
    "# data_path = \"squad-stratified-100-ft-v1.json\"\n",
    "\n",
    "# # Load dataset\n",
    "# with open(data_path) as f:\n",
    "#     dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# # We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# # Initial dataset stats\n",
    "# print(\"Num examples:\", len(dataset))\n",
    "# print(\"First example:\")\n",
    "# for message in dataset[0][\"messages\"]:\n",
    "#     print(message)\n",
    "\n",
    "# # Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# # Format error checks\n",
    "# format_errors = defaultdict(int)\n",
    "\n",
    "# for ex in dataset:\n",
    "#     if not isinstance(ex, dict):\n",
    "#         format_errors[\"data_type\"] += 1\n",
    "#         continue\n",
    "\n",
    "#     messages = ex.get(\"messages\", None)\n",
    "#     if not messages:\n",
    "#         format_errors[\"missing_messages_list\"] += 1\n",
    "#         continue\n",
    "\n",
    "#     for message in messages:\n",
    "#         if \"role\" not in message or \"content\" not in message:\n",
    "#             format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "#         if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "#             format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "#         if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "#             format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "#         content = message.get(\"content\", None)\n",
    "#         if not content or not isinstance(content, str):\n",
    "#             format_errors[\"missing_content\"] += 1\n",
    "\n",
    "#     if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "#         format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "# if format_errors:\n",
    "#     print(\"Found errors:\")\n",
    "#     for k, v in format_errors.items():\n",
    "#         print(f\"{k}: {v}\")\n",
    "# else:\n",
    "#     print(\"No errors found\")\n",
    "\n",
    "# # Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# # Token counting functions\n",
    "# encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# # not exact!\n",
    "# # simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "# def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "#     num_tokens = 0\n",
    "#     for message in messages:\n",
    "#         num_tokens += tokens_per_message\n",
    "#         for key, value in message.items():\n",
    "#             num_tokens += len(encoding.encode(value))\n",
    "#             if key == \"name\":\n",
    "#                 num_tokens += tokens_per_name\n",
    "#     num_tokens += 3\n",
    "#     return num_tokens\n",
    "\n",
    "# def num_assistant_tokens_from_messages(messages):\n",
    "#     num_tokens = 0\n",
    "#     for message in messages:\n",
    "#         if message[\"role\"] == \"assistant\":\n",
    "#             num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "#     return num_tokens\n",
    "\n",
    "# def print_distribution(values, name):\n",
    "#     print(f\"\\n#### Distribution of {name}:\")\n",
    "#     print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "#     print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "#     print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "# # Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# # Warnings and tokens counts\n",
    "# n_missing_system = 0\n",
    "# n_missing_user = 0\n",
    "# n_messages = []\n",
    "# convo_lens = []\n",
    "# assistant_message_lens = []\n",
    "\n",
    "# for ex in dataset:\n",
    "#     messages = ex[\"messages\"]\n",
    "#     if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "#         n_missing_system += 1\n",
    "#     if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "#         n_missing_user += 1\n",
    "#     n_messages.append(len(messages))\n",
    "#     convo_lens.append(num_tokens_from_messages(messages))\n",
    "#     assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "# print(\"Num examples missing system message:\", n_missing_system)\n",
    "# print(\"Num examples missing user message:\", n_missing_user)\n",
    "# print_distribution(n_messages, \"num_messages_per_example\")\n",
    "# print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "# print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "# n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "# print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# # Pricing and default n_epochs estimate\n",
    "# MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "# MIN_TARGET_EXAMPLES = 100\n",
    "# MAX_TARGET_EXAMPLES = 25000\n",
    "# TARGET_EPOCHS = 3\n",
    "# MIN_EPOCHS = 1\n",
    "# MAX_EPOCHS = 25\n",
    "\n",
    "# n_epochs = TARGET_EPOCHS\n",
    "# n_train_examples = len(dataset)\n",
    "# if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "#     n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "# elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "#     n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "# n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "# print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "# print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "# print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "# print(\"See pricing page to estimate total costs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the Fine-Tuning data to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = openai.File.create(\n",
    "  file=open(\"squad-stratified-100-ft-v1.json\", \"r\"),\n",
    "  purpose='fine-tune',\n",
    "\n",
    ")\n",
    "\n",
    "while file_object['status'] != 'processed':\n",
    "    file_object = openai.File.retrieve(file_object['id'])\n",
    "    time.sleep(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Fine Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_job = openai.FineTuningJob.create(training_file=file_object['id'], model=\"gpt-3.5-turbo\", suffix=\"v1\")\n",
    "while openai.FineTuningJob.retrieve(ft_job[\"id\"]).fine_tuned_model == None:\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = openai.FineTuningJob.retrieve(ft_job[\"id\"]).fine_tuned_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you answer the following question based on the given context? If not, say, I don't know:\\n\\nQuestion: What is the capital of France?\\n\\nContext: The capital of Mars is Gaia. Answer:\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "### Get Answers from the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ft_generated_answer\"] = df.progress_apply(answer_question, model=model_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "finetuned_model_evaluator = ConfusionMatrixEvaluator(df, answers_column=\"ft_generated_answer\")\n",
    "\n",
    "# Run the evaluation\n",
    "finetuned_model_evaluator.evaluate_answers()\n",
    "finetuned_model_error_categories = finetuned_model_evaluator.generate_matrices(use_percentages=True)\n",
    "finetuned_model_error_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the results to a JSON file\n",
    "# df.to_json(\"1K_with_ft_generated_answers.json\", lines=True, orient=\"records\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_matrix(df, answers_column):\n",
    "    \"\"\"\n",
    "    Evaluate the confusion matrix for a given DataFrame and answer column.\n",
    "    \"\"\"\n",
    "    evaluator = ConfusionMatrixEvaluator(df, answers_column=answers_column)\n",
    "    evaluator.evaluate_answers()\n",
    "    matrix = evaluator.generate_matrices(use_percentages=True)\n",
    "    return matrix\n",
    "\n",
    "def plot_overall_error(matrix1, matrix2, label1, label2):\n",
    "    \"\"\"\n",
    "    Plot a bar chart showing only the overall error between two confusion matrices.\n",
    "    \"\"\"\n",
    "    # Calculate overall error\n",
    "    error_categories = ['Expected but Wrong', 'Hallucination']\n",
    "    matrix1_error = sum([float(matrix1.loc[cat].replace('%', '')) for cat in error_categories])\n",
    "    matrix2_error = sum([float(matrix2.loc[cat].replace('%', '')) for cat in error_categories])\n",
    "    \n",
    "    labels = ['Overall Error']\n",
    "    matrix1_values = [matrix1_error]\n",
    "    matrix2_values = [matrix2_error]\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, matrix1_values, width, label=label1)\n",
    "    rects2 = ax.bar(x + width/2, matrix2_values, width, label=label2)\n",
    "    \n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_title('Overall Error Comparison between {} and {}'.format(label1, label2))\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot only the overall error\n",
    "plot_overall_error(matrix_plain, matrix_ft, \"gpt-3.5-turbo-0613\", \"FineTuned gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Learning with Qdrant to Improve RAG\n",
    "\n",
    "So far, we've been using the OpenAI model to answer questions where the answer is present in the context. But what if we want to answer questions where the answer is not present in the context? This is where few-shot learning comes in. Few-shot learning is a type of transfer learning that allows us to answer questions where the answer is not present in the context. We can do this by providing a few examples of the answer we're looking for, and the model will learn to answer questions where the answer is not present in the context.\n",
    "\n",
    "## Finetuning and Inference Changes\n",
    "We will use the same dataset as before, but this time we will only provide a few examples of the answer we're looking for. We will also provide a few examples of the answer we're not looking for. This will allow the model to learn to handle questions where the answer is not present in the context. \n",
    "\n",
    "We assumed perfect retrieval in the previous section, but we will not assume perfect retrieval here. Instead, we will use a vector search engine to find similar questions and answers, and then use those to finetuning the model. We will use [Qdrant](https://qdrant.tech/), an open-source vector search engine. We will use Qdrant to find similar questions and answers, and then use those to finetuning the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
